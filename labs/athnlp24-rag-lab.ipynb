{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# AthNLP 2024 Retrieval Augmentation"
   ],
   "metadata": {
    "id": "ogrxuWG-vdJ3"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part 1 - Prompting\n",
    "Get access token, load pre-trained quantized LlaMA 3.1, use it e.g. in PubMedQA to answer questions (small portion of the training set). Try again using [chain-of-thought prompting](https://arxiv.org/abs/2201.11903), using [self-explanations](https://aclanthology.org/2024.findings-acl.19/), [faithful or not](https://arxiv.org/pdf/2407.14487), each student has to think his/her own alternatives. Evaluate on a benchmark (we will give accuracy / students need to implement [keystroke reduction](https://arxiv.org/pdf/2006.12040)). Try again using [self-consistency](https://arxiv.org/pdf/2203.11171) instead of greedy decoding e.g. based on ensembles of the different attempts (prompts). Give the same prompts to ChatGPT for the first 10 examples and compare the results using both evaluation metrics.\n",
    "\n",
    "To continue with this lab you need to create a [HF access token](https://huggingface.co/docs/hub/en/security-tokens)."
   ],
   "metadata": {
    "id": "94ZHNmwsq4ia"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "from huggingface_hub import login\n",
    "from tqdm import tqdm\n",
    "import evaluate\n",
    "\n",
    "# Generate a Hugging Face access token and install LlaMA 3.1\n",
    "from huggingface_hub import login\n",
    "os.environ[\"HF_ACCESS_TOKEN\"] = \"WRITE_YOUR_ACCESS_TOKEN_HERE]\"\n",
    "login(token=os.environ[\"HF_ACCESS_TOKEN\"], add_to_git_credential=True)\n",
    "\n",
    "llm = LLM(llm_name=\"meta-llama/Meta-Llama-3.1-8B-Instruct\", quantized=True)\n",
    "toy = llm.answer(\"When was superconductivity discovered?\", num_beams=1)"
   ],
   "metadata": {
    "id": "fvzn_rFv4aXR"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Load the dataset (PubMedQA)\n",
    "pubmedqa_train_data = DataLoaderPQAInferenceWithAugmentations(boundaries=[0, 990])\n",
    "pubmedqa_test_data = DataLoaderPQAInferenceWithAugmentations(boundaries=[990, 1000])\n",
    "\n",
    "# Create appropriate (torch) Data Loaders\n",
    "train_loader = torch.utils.data.DataLoader(pubmedqa_train_data, batch_size=1, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(pubmedqa_test_data, batch_size=1, shuffle=False)\n",
    "\n",
    "save_dir = \"./results\""
   ],
   "metadata": {
    "id": "gIqdVNnEfv6t"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Prompt it to answer the questions (without extra information in the prompt)\n",
    "predicted_llm = {}\n",
    "for i, (question, options, concepts, answers, qid) in tqdm(enumerate(test_loader, 0)):\n",
    "  if i == 0:\n",
    "    answer = llm.answer(question=question[0], num_beams=1)\n",
    "    predicted_llm[qid[0]] = answer\n",
    "write_dict(predicted_llm, save_dir, \"answers_dict_llm_demo.txt\")\n",
    "\n",
    "predicted_llm[list(predicted_llm.keys())[0]]"
   ],
   "metadata": {
    "id": "GvUmbmmwfSm0"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Prompt it to answer the questions (asking for a structured output)\n",
    "# Structured output is useful. Otherwise we won't easily detect the position of the exact answer and of the explanation.\n",
    "medrag_cot = MedRAG(llm_name=\"meta-llama/Meta-Llama-3.1-8B-Instruct\", rag=False, retriever_name=\"MedCPT\",\n",
    "                    corpus_name=\"Textbooks\", quantized=True)\n",
    "\n",
    "predicted_cot1 = {}\n",
    "for i, (question, options, concepts, answers, qid) in tqdm(enumerate(test_loader, 0)):\n",
    "    answer, snippets, scores = medrag_cot.answer(question=question[0], options=options, k=0, num_beams=1)\n",
    "    predicted_cot1[qid[0]] = answer\n",
    "write_dict(predicted_cot1, save_dir, \"answers_dict_cot1_demo.txt\")\n",
    "\n",
    "predicted_cot1[list(predicted_cot1.keys())[0]]"
   ],
   "metadata": {
    "id": "bbrwWdetffhX"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Prompt it with CoT\n",
    "predicted_cot2 = {}\n",
    "\n",
    "# ..."
   ],
   "metadata": {
    "id": "AXd2E_fMfUg8"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Define some helpers for model evaluation\n",
    "\"\"\"\n",
    "Removes all special characters.\n",
    "\"\"\"\n",
    "def clean(answer):\n",
    "    return re.sub(\" +\", \"\", re.sub(r'[^A-Za-z.]+', ' ', answer))\n",
    "\n",
    "\"\"\"\n",
    "Loads the models' answers and explanations.\n",
    "\"\"\"\n",
    "def eval_prediction(prediction):\n",
    "    if isinstance(prediction[list(prediction.keys())[0]], str):\n",
    "        explanations = {\n",
    "            qid: prediction[qid].split(\"\\\"step_by_step_thinking\\\":\")[-1].split(\"\\\"answer_choice\\\":\")[0].strip()\n",
    "            for qid in prediction.keys()}\n",
    "        answers = {qid: clean(prediction[qid].split(\"\\\"answer_choice\\\":\")[-1].strip())[0]\n",
    "                             for qid in prediction.keys()}\n",
    "    elif isinstance(prediction[list(prediction.keys())[0]], list):\n",
    "        explanations = {\n",
    "            qid: prediction[qid][0].split(\"\\\"step_by_step_thinking\\\":\")[-1].split(\"\\\"answer_choice\\\":\")[0].strip()\n",
    "            for qid in prediction.keys()}\n",
    "        answers = {qid: clean(prediction[qid][0].split(\"\\\"answer_choice\\\":\")[-1].strip())[0]\n",
    "                             for qid in prediction.keys()}\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    return explanations, answers"
   ],
   "metadata": {
    "id": "MbLhyacGD0BE"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Prompt it to produce self-explanations\n",
    "predicted_exp = {}\n",
    "\n",
    "# ..."
   ],
   "metadata": {
    "id": "wwE5UciGfSpt"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Evaluate your results on a benchmark (accuracy / keystroke reduction)\n",
    "explanations, answers = eval_prediction(predicted_cot1)\n",
    "\n",
    "mapping = {'yes': 'A', 'no': 'B', 'maybe': 'C'}\n",
    "ground_truth = {pubmedqa_test_data.qids[idx]: mapping[pubmedqa_test_data.choices[idx]]\n",
    "                for idx in range(len(pubmedqa_test_data.qids))}\n",
    "accuracy = [True if answers[qid] == ground_truth[qid] else False\n",
    "            for qid in answers.keys()].count(True) / len(pubmedqa_test_data.qids)\n",
    "\n",
    "keystroke_reduction = {}\n",
    "# ..."
   ],
   "metadata": {
    "id": "CzH-2esmfS37"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Implement self-consistency and compare the results\n",
    "predicted_selfcon = {}\n",
    "\n",
    "# ..."
   ],
   "metadata": {
    "id": "1e6wnexufS8K"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Prompt ChatGPT and compare the results\n",
    "predicted_gpt4o = {}\n",
    "\n",
    "# ..."
   ],
   "metadata": {
    "id": "r2wfsiT5prb9"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part 2 - LearningÂ (in context)\n",
    "Prompt again the LLM using training examples as few-shot demonstrators; which are selected in different ways: **(a)** Retrieve from the training data similar questions using cosine similarity. **(b)** Prompt an LLM (e.g. ChatGPT or LlaMA 3.1) to predict concepts from the tag set provided. Repeat the previous step by retrieving demonstrators based on the tags (based on the assumption that the same tags are included in similar examples). For each of the above scenarios, use the answers to add demonstrators in the context. We will give as an example retrieving similar examples using gold (human-authored) tags, the students have to follow the same methodology (i.e. [MedRAG](https://teddy-xionggz.github.io/benchmark-medical-rag/)) or use simple prompting using cosine similarity and tags predicted by an LLM (as tagger).\n",
    "- Evaluate again using accuracy (provided) and [keystroke reduction](https://arxiv.org/pdf/2006.12040) (implemented above) and compare to vanilla prompting.\n",
    "- Visualise performance against the number of shots (amount of training examples included in the prompt as demonstrators). Profile the time needed and think of possible efficiency tricks (e.g: how efficient is cosine in terms of matrix multipications in the accelerator and how can we improve it?)."
   ],
   "metadata": {
    "id": "fJetz5SirWO-"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Retrieve from the training data similar questions using cosine similarity.\n",
    "neighbors = {}\n",
    "for i, (question, options, concepts, answers, qid) in tqdm(enumerate(test_loader, 0)):\n",
    "  neighbors[qid] = {}\n",
    "  for j, (question_neig, options_neig, concepts_neig, answers_neig, qid_neig) in enumerate(train_loader, 0):\n",
    "    sim = 0\n",
    "    # ..."
   ],
   "metadata": {
    "id": "g65EXbBLrwQw"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Prompt an LLM to predict concepts from the tag set.\n",
    "concepts_unique = []\n",
    "for instance in pubmedqa_train_data.concepts + pubmedqa_test_data.concepts:\n",
    "    concepts_unique = concepts_unique + [concept for concept in instance]\n",
    "concepts_unique = list(set(concepts_unique))\n",
    "\n",
    "# ..."
   ],
   "metadata": {
    "id": "P_dukmv1f29z"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Repeat the previous step by retrieving demonstrators based on the tags.\n",
    "neighbors = {}\n",
    "for i, (question, options, concepts, answers, qid) in tqdm(enumerate(test_loader, 0)):\n",
    "  neighbors[qid[0]] = []\n",
    "  for j, (question_neig, options_neig, concepts_neig, answers_neig, qid_neig) in enumerate(train_loader, 0):\n",
    "    if bool(set([c[0] for c in concepts]) & set([c[0] for c in concepts])):\n",
    "        neighbors[qid[0]].append(qid_neig[0])\n",
    "    neighbors[qid[0]] = list(set(neighbors[qid[0]]))\n",
    "\n",
    "neighbors[list(neighbors.keys())[0]]"
   ],
   "metadata": {
    "id": "yRMUM6eAgFK1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# For each of the above scenarios, use the answers to add demonstrators in the context.\n",
    "# We demonstrate retrieving similar examples using gold (human-authored) tags,\n",
    "# you have to follow the same methodology using cosine similarity and tags\n",
    "# predicted by an LLM (as tagger).\n",
    "num_shots = 3\n",
    "predicted_fs = {}\n",
    "for i, (question, options, concepts, answers, qid) in tqdm(enumerate(test_loader, 0)):\n",
    "   shots_selected = random.sample(neighbors[qid[0]], num_shots)\n",
    "   query = \"\"\n",
    "   for shot in shots_selected:\n",
    "       query += (pubmedqa_train_data.data.data[shot]['QUESTION'] + \" {\\\"step_by_step_thinking\\\": \\\"\" +\n",
    "                 pubmedqa_train_data.data.data[shot]['LONG_ANSWER'] + \"\\\"answer_choice\\\": \\\"\" +\n",
    "                 mapping[pubmedqa_train_data.data.data[shot]['final_decision']] + \"\\\"}\\n \\n\")\n",
    "   query += question[0]\n",
    "\n",
    "   answer, snippets, scores = medrag_cot.answer(question=question[0], options=options, k=0, num_beams=1)\n",
    "   predicted_fs[qid[0]] = answer\n",
    "\n",
    "predicted_fs[list(predicted_fs.keys())[0]]\n",
    "\n",
    "# Use a tagger (e.g. LLM-based) to label DB and test;\n",
    "# retrieve related training records based on the assigned tags;\n",
    "# compute Keystroke Reduction as an evaluation metric\n",
    "# ..."
   ],
   "metadata": {
    "id": "Qtu1IRKwf3ur"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Evaluate and compare to vanilla prompting\n",
    "explanations, answers = eval_prediction(predicted_fs)\n",
    "\n",
    "mapping = {'yes': 'A', 'no': 'B', 'maybe': 'C'}\n",
    "ground_truth = {pubmedqa_test_data.qids[idx]: mapping[pubmedqa_test_data.choices[idx]]\n",
    "                for idx in range(len(pubmedqa_test_data.qids))}\n",
    "accuracy_fs = [True if answers[qid] == ground_truth[qid] else False\n",
    "               for qid in answers.keys()].count(True) / len(pubmedqa_test_data.qids)\n",
    "\n",
    "# ...\n",
    "\n",
    "titles = [\"Vanilla prompting\", \"Few shot (3 random shots from gold tags)\"]\n",
    "metrics = [accuracy, accuracy_fs] # ...\n",
    "\n",
    "plt.figure(figsize = (10, 5))\n",
    "plt.bar(titles, metrics, color ='maroon', width = 0.4)\n",
    "plt.xlabel(\"Method\")\n",
    "plt.ylabel(\"Acccuracy\")\n",
    "plt.title(\"Factuality comparison (in terms of accuracy)\")\n",
    "plt.show()\n",
    "\n",
    "# ..."
   ],
   "metadata": {
    "id": "DvBeXaQ6f65E"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Visualise performance against the number of shots\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ..."
   ],
   "metadata": {
    "id": "oBTOfWvff7Gk"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Profile the time needed (efficiency tricks: cosine/dot)\n",
    "import time\n",
    "\n",
    "# ..."
   ],
   "metadata": {
    "id": "QYTJ9lBnf8tt"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part 3 - Retrieval Augmentation (frozen and with guided decoding)\n",
    "Setup a FAISS index with dense representations of Medical Textbooks (no labels) and use it to update the prompt with external information. We will provide an implementation example based on [MedRAG toolkit](https://teddy-xionggz.github.io/benchmark-medical-rag/). We also give  answers from the LLM using different amounts of retrieved documents and document collections (PubMed articles) since indexing and retrieval take time. Evaluate again the Retrieval Augmented LLM's responses compared to vanilla and few-shot and visualise performance compared to # retrieved records. Instead of using frozen retrieval augmentation, try again with guided-decoding based on the tags' representations, where the beam decoder is influenced by the tags' vector representations when producing an output sequence as it is described in [DMMCS framework](https://aclanthology.org/2024.findings-acl.444/). Retrieve related training records based on the assigned tags (as in Part 2) using either the gold tags or those generated using the LLM tagger previously developed and apply DMMCS by retrieving records with similar tags. Measure the correlation between answers."
   ],
   "metadata": {
    "id": "szjQ9xfarwqv"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Setup a DB with dense representations (no labels)\n",
    "medrag = MedRAG(llm_name=\"meta-llama/Meta-Llama-3.1-8B-Instruct\", rag=True, retriever_name=\"MedCPT\",\n",
    "                corpus_name=\"Textbooks\", quantized=True)`"
   ],
   "metadata": {
    "id": "Olfj7ytqtjtN"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Use to update the prompt with external information\n",
    "num_shots = 3\n",
    "predicted_rag = {}\n",
    "for i, (question, options, concepts, answers, qid) in tqdm(enumerate(test_loader, 0)):\n",
    "    answer, snippets, scores = medrag.answer(question=question[0], options=options, k=num_shots, num_beams=1)\n",
    "    predicted_rag[qid[0]] = answer\n",
    "write_dict(predicted_rag, save_dir, \"answers_dict_rag_demo.txt\")\n",
    "\n",
    "predicted_rag[list(predicted_rag.keys())[0]]"
   ],
   "metadata": {
    "id": "j_n90lStoEHN"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Assess compared to vanilla and few-shot\n",
    "explanations, answers = eval_prediction(predicted_rag)\n",
    "\n",
    "mapping = {'yes': 'A', 'no': 'B', 'maybe': 'C'}\n",
    "ground_truth = {pubmedqa_test_data.qids[idx]: mapping[pubmedqa_test_data.choices[idx]]\n",
    "                for idx in range(len(pubmedqa_test_data.qids))}\n",
    "accuracy_rag = [True if answers[qid] == ground_truth[qid] else False\n",
    "                for qid in answers.keys()].count(True) / len(pubmedqa_test_data.qids)\n",
    "\n",
    "# ...\n",
    "\n",
    "titles = [\"Vanilla prompting\", \"Few shot (3 random shots from gold tags)\", \"RAG (3 random relevant snippets)\"]\n",
    "metrics = [accuracy, accuracy_fs, accuracy_rag] # ...\n",
    "\n",
    "plt.figure(figsize = (10, 5))\n",
    "plt.bar(titles, metrics, color ='maroon', width = 0.4)\n",
    "plt.xlabel(\"Method\")\n",
    "plt.ylabel(\"Acccuracy\")\n",
    "plt.title(\"Factuality comparison (in terms of accuracy)\")\n",
    "plt.show()\n",
    "\n",
    "# ..."
   ],
   "metadata": {
    "id": "fb3rRyWSoEJ9"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Visualise performance compared to # retrieved records\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ..."
   ],
   "metadata": {
    "id": "V1627qAWoEMm"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Apply DMM by retrieving records with similar tags.\n",
    "!python datagen_dmmcs.py\n",
    "!python stats_extraction.py --config ../config/stats_extractor_config_pubmedqa.json\n",
    "\n",
    "dmm_config = {\n",
    "    \"dataset_name\": \"pubmedqa_20ex\",\n",
    "    \"dataset_concepts_mapper\": \"./pubmedqa_dmmcs_data/mapping.csv\",\n",
    "    \"hist_file_path\": \"./snapshots/artifacts/hist_train.pkl\",\n",
    "    \"mmc_sim_file_path\": \"./snapshots/artifacts/median_max_cos_c.pkl\",\n",
    "    \"word_index_path\": \"./snapshots/artifacts/word_index.pkl\",\n",
    "    \"embedding_matrix_path\": \"./snapshots/artifacts/embedding_matrix.npy\",\n",
    "    \"n_gpu\": 1,\n",
    "    \"cuda_nr\": 0,\n",
    "    \"seed\": 42,\n",
    "    \"num_workers_test\": 4,\n",
    "    \"dmmcs_params\": {\n",
    "                      \"do_dmmcs\": True,\n",
    "                      \"alpha\": 0.15\n",
    "                    },\n",
    "    \"generation_params\":  {\n",
    "                          \"do_sample\": False,\n",
    "                          \"num_beams\": 1,\n",
    "                          \"max_length\": 8192,\n",
    "                          \"min_length\": 5\n",
    "                          },\n",
    "    \"logging\":  {\n",
    "                \"print_on_screen\": True\n",
    "                }\n",
    "}\n",
    "\n",
    "# Instantiate InstructBLIP with the provided config file\n",
    "llama_dmmcs_config = DMMLM(dmm_config, llm_name=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "                           token=os.environ[\"HF_ACCESS_TOKEN\"])\n",
    "\n",
    "# Run!\n",
    "predicted_dmmcs, actuals = llama_dmmcs_config.generate(test_loader, llama_dmmcs_config.config[\"hist_file_path\"],\n",
    "                                                   llama_dmmcs_config.config[\"mmc_sim_file_path\"])\n",
    "\n",
    "write_dict(predicted_dmmcs, save_dir, \"answers_dict_dmmcs_demo.txt\")\n",
    "\n",
    "predicted_dmmcs[list(predicted_dmmcs.keys())[0]]"
   ],
   "metadata": {
    "id": "5DarlnCboERQ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Measure the correlation between answers.\n",
    "class Correlation():\n",
    "    def __init__(self, device='cuda:0'):\n",
    "        super().__init__()\n",
    "        self.metrics = evaluate.load(\"bertscore\")\n",
    "        self.device = device\n",
    "\n",
    "    \"\"\"\n",
    "    Evaluate predictions using scores from BERToids.\n",
    "    \"\"\"\n",
    "    def evaluate(self, refs, hyps):\n",
    "        self.metrics.add_batch(predictions=refs, references=hyps)\n",
    "        results = self.metrics.compute(model_type='albert-base-v2', num_layers=5, all_layers=False, idf=False,\n",
    "                                       lang='en', rescale_with_baseline=True, baseline_path=None)\n",
    "        return np.mean(results[\"f1\"])\n",
    "\n",
    "# ..."
   ],
   "metadata": {
    "id": "qcKb9nyuoEUC"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Concept-based image captioning\n",
    "Download [IU-XRAY dataset](https://www.kaggle.com/datasets/raddar/chest-xrays-indiana-university) for medical image captioning ([diagnostic captioning](https://arxiv.org/pdf/2101.07299)) and use DMMCS for guided decoding of a Vision Language Model (e.g. IDEFICS-2, IDEFICS-3, OpenFlamingo) based on [our implementation](https://github.com/nlpaueb) of the framework on InstructBLIP. Measure BERTscore and compare LM perplexities."
   ],
   "metadata": {
    "id": "DEz0hg9UdmXl"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%cd ..\n",
    "!git clone https://github.com/nlpaueb/dmmcs.git\n",
    "% cd dmmcs\n",
    "\n",
    "# ..."
   ],
   "metadata": {
    "id": "ZuqjZ4LYdmex"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "L4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
